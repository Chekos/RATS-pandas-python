{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pandas logo](https://pandas.pydata.org/_static/pandas_logo.png)\n",
    "# Pandas\n",
    "\n",
    "> *`pandas` is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.* - [pandas.pydata.org](https://pandas.pydata.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Highlights\n",
    "* <mark>A fast and efficient **DataFrame** object for data manipulation with integrated indexing;</mark>\n",
    "* <mark>Tools for reading and writing data between in-memory data structures and different formats: CSV and text files, Microsoft Excel, SQL databases, and the fast HDF5 format;</mark>\n",
    "* Intelligent data alignment and integrated handling of missing data: gain automatic label-based alignment in computations and easily manipulate messy data into an orderly form;\n",
    "* <mark>Flexible reshaping and pivoting of data sets;</mark>\n",
    "* <mark>Intelligent label-based slicing, fancy indexing, and subsetting of large data sets;</mark>\n",
    "* <mark>Columns can be inserted and deleted from data structures for size mutability;</mark>\n",
    "* <mark>Aggregating or transforming data with a powerful group by engine allowing split-apply-combine operations on data sets;</mark>\n",
    "* High performance merging and joining of data sets;\n",
    "* Hierarchical axis indexing provides an intuitive way of working with high-dimensional data in a lower-dimensional data structure;\n",
    "* Time series-functionality: date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging. Even create domain-specific time offsets and join time series without losing data;\n",
    "* Highly optimized for performance, with critical code paths written in Cython or C.\n",
    "* <mark>Python with pandas is in use in a wide variety of academic and commercial domains, including Finance, Neuroscience, Economics, Statistics, Advertising, Web Analytics, and more.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Installation\n",
    "If you use **anaconda** you probably already have `pandas`. It's one of the 1000s of packages already included in your download.\n",
    "\n",
    "If you use **anaconda** and don't have `pandas` you can easily install it with:<br>\n",
    "`conda install pandas` from a **terminal** or <br>`!conda install pandas -y` from within a `jupyter notebook` cell.\n",
    "\n",
    "you can also use another popular python package manager `pip`:\n",
    "`pip install pandas` from a **terminal** or <br>`!pip install pandas` from within a `jupyter notebook` cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__note__: If you're wondering why does the `jupyter notebook` command has a `!` before it it's because `jupyter` is running a `python` kernel but what you need is to tell your computer (not `python`) to run the command and adding a `!` tells `jupyter` to send that command out of `python` and to the computer rather than to try to run it as a `python` command.<br>\n",
    "the `-y` at the end of the `conda install` command means \"when the computer asks me something tell it I said 'yes'\". this is important if you're installing something using `conda` from within a `jupyter notebook` because before you download anything it'll ask you to confirm that you do indeed want to download said package (in this case `pandas` and it's _dependencies_ [other `python` libraries it depends on]). If you are running the code in a `jupyter notebook` cell you have no way to confirm directly to the `terminal` so you send the confirmation along with the command. <br>\n",
    "If you use `pip` to install any package it'll just do it, they don't care about your confirmation. You asked for it, you get it. \n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading and writing data\n",
    "\n",
    "`pandas` uses the methods `.from_***()` and `.to_***()` to read and write data where `***` is one of the different types of data formats.\n",
    "\n",
    "```python\n",
    "import pandas as pd ## this is a convention. It just saves time, 66.7% to be exact.\n",
    "\n",
    "# we store the data in a DataFrame. Conventionally, you'd name it 'df' but it can be anything you want.\n",
    "df = pd.read_csv(\"path_to_your_file.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**cool fact**: You can save your `clipboard` into a pandas dataframe with `pd.read_clipboard()`. So if you go to a website and they have their data written out but not available to download you can highlight > copy >  and `pd.read_clipboard()` > do your analysis > save it to an excel file or csv or .dta or whatever you want.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slicing/filtering datasets\n",
    "\n",
    "Once you have your dataset set up you may want to use only a few columns or rows. In `pandas` you can `slice` the dataframe in a few different ways.\n",
    "\n",
    "* `.loc[]` and `.iloc[]` notation.\n",
    "* boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/Bee Colony Census Data by County.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "```python\n",
    "df.head()\n",
    "```\n",
    "will show you the first rows of your __`dataframe`__ (`.tail()` shows you the last rows). By default, it shows 5 but you can change it to whatever you'd like by *changing the parameter __`n`__*\n",
    "\n",
    "to explore this more type out `df.head()` and **between the parentheses** press `shift + tab`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### .iloc[] / .loc[]\n",
    "\n",
    "Both `.iloc[]` and `.loc[]` accept a _row indexer_ and a _column indexer_ as parameters like this:\n",
    "```python\n",
    "df.iloc[4, 5]\n",
    "df.loc[df['State'] == 'CALIFORNIA', 'County']\n",
    "```\n",
    "`.iloc[]` is integer based and locates the i-th row in your __dataframe__ \n",
    "`.loc[]` is primarily label based. \n",
    "\n",
    "This depends on the values of your `index` in your __dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your `index` is integer-based you can use `.iloc[]` to access it's rows and columns. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4, 3:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This translates to \"using the __dataframe__ `df` locate the 4th row and the 3-8th columns (ending column not included)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your index was label-based you could use `.loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.set_index('State')\n",
    "\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.loc['CALIFORNIA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### boolean indexing / filtering\n",
    "\n",
    "Another way of grabbing subsets of your __dataframe__ is by using _boolean indexing_. Basically, logic conditions. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ag District'] == 'SAN JOAQUIN VALLEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This long __series__ of `true` and `false` can work as a filter. Essentially saying \"show me all the rows where _X condition_ is true\".\n",
    "\n",
    "The notation is as follows:\n",
    "```python\n",
    "df[df[column] == value] \n",
    "```\n",
    "where `column` is one of your column names and `value` is one of the values found in that column. Common operators are `< > == !=`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our condition before was\n",
    "```python\n",
    "df['Ag District'] == 'SAN JOAQUIN VALLEY'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Ag District'] == 'SAN JOAQUIN VALLEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note**: This is a \"view\" of the original __dataframe__ `df` and can be stored in another variable. \n",
    "```python\n",
    "san_joaquin_df = df[df['Ag District'] == 'SAN JOAQUIN VALLEY']\n",
    "```\n",
    "However, you __cannot__ edit the data in the original __dataframe__ `df` by editing `san_joaquin_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_joaquin_df = df[df['Ag District'] == 'SAN JOAQUIN VALLEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_joaquin_df['County ANSI'] = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_joaquin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Ag District'] == 'SAN JOAQUIN VALLEY'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You can use multiple conditions to subset a __dataframe__\n",
    "```python\n",
    "df[(df['Ag District'] == 'SAN JOAQUIN VALLEY') & (df['Year'] == 2012)]\n",
    "```\n",
    "Just make sure to use __parentheses__ around each condition and use `&` for 'and' and `|` for 'or'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grabbing a subset of columns\n",
    "\n",
    "You can access a column of a `pandas` __dataframe__ like this `df[column_name]`.\n",
    "```python\n",
    "df['County']\n",
    "```\n",
    "This returns all the values of that column. Because __dataframes__ are composed by __series__, when you access one column of a __dataframe__ you'll get a __series__ in return. __dataframes__ and __series__ have different properties. We'll be working mostly with __dataframes__. \n",
    "\n",
    "To grab more than one column you must pass a `list` with the column names you'd like to access. For example,\n",
    "```python\n",
    "df[['State','County', 'Value', 'Year']]\n",
    "```\n",
    "Notice the double brackets. `lists` in `python` use `[]` and to access a column in a __dataframe__ you use `[]` too. This is why you end up with two sets of brackets. Another way to code this would be,\n",
    "```python\n",
    "columns_i_need = ['State', 'County', 'Value', 'Year']\n",
    "\n",
    "df[columns_i_need]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Notice__ that you the columns in the new __dataframe__ are in the order you specified in your list and not the original order. This is because your new __dataframe__ is being build one step at a time by going back to the original __dataframe__ and grabbing the column that matches your lists' value. <br>\n",
    "You could in theory pass the `list` `columns_i_need = ['State', 'Year', 'County', 'Year', 'Value', 'Year']` and your new __dataframe__ would have the 6 columns in that specific order. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### series and `dtypes`\n",
    "\n",
    "In `pandas` a __series__ is a data structure that represents a column in a __dataframe__, the most basic unit of a __dataframe__. `stata`, for example, doesn't have a separate data structure for this.\n",
    "Here's a labeled closer look into the components of a __dataframe__.\n",
    "![dataframe_df](../images/anatomy_df.png)\n",
    "\n",
    "\n",
    "here `director_name`, `duration`, `imdb_score` are all __series__. Each series can have a `dtype`. There are a few different `dtypes` in `pandas`. There are those common across programming languages like `bool`, `int` and `float`. In `pandas` string __series__ are of `dtype` `object` which is a blanket type for most data that is non-numeric. `pandas` also has `datetime` and `categorical` __series__ which have their own special properties.\n",
    "\n",
    "To _access_ each `dtypes` properties we use `accessors`. For example, to access the `string` properties of a column, which means you can apply `string` methods to each value in that __series__, you would do the following:\n",
    "```python\n",
    "df['State'].str.upper()\n",
    "```\n",
    "`.str` allows us to `access` the string methods like `.upper()`, `.capitalize()`, `.split()`, `.strip()` and apply them to each of the values in `df['State']`.<br>\n",
    "Try below, grab a column (or __series__) and use some string methods to change the values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tip__: when you apply these methods to a __series__ you are not actually changing the values of that __series__ themselves. You are telling `pandas` \"grab the column XX in __dataframe__ `df` apply this `.str` method, give me __that__ series you just made.\" <br>\n",
    "To clean up the data __in__ the __dataframe__ you need to _reassign_ the values of that __series__. For example,\n",
    "```python\n",
    "df['State'].str.lower() # this would give me back a series with all the values lowercased\n",
    "\n",
    "df['State'] = df['State'].str.lower() # this takes that new series and assigns it to the column 'State'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouping / aggregating\n",
    "\n",
    "to aggregate values by a group you can use `.groupby()`. For example,\n",
    "```python\n",
    "df.groupby('State')\n",
    "```\n",
    "If you run this command you'll get something like this `<pandas.core.groupby.groupby.DataFrameGroupBy object at 0x0000000007F3BE10>` which means a `groupby` object was created. To do anything useful with it we apply aggregators to it. For example, \n",
    "```python\n",
    "df.groupby('State').sum()\n",
    "```\n",
    "would return the sum of all the numeric values in your __dataframe__ by 'State'.\n",
    "If you want to look at a specific value only you can grab that column like this:\n",
    "```python\n",
    "df.groupby('State')['Value'].sum()\n",
    "```\n",
    "Use a `list` if you want to grab multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__note__: there are multiple `aggreagators` like `.sum()`. You can use:\n",
    "* `.mean()`\n",
    "* `.median()`\n",
    "* `.min()`\n",
    "* `.max()` \n",
    "* `.count()` \n",
    "\n",
    "and many others. You will not remember these so you can always google them or _assign_ your `groupby` object to a `variable` and explore your options with `tab` like we explored in the [first notebook](00_Intro.ipynb).\n",
    "```python\n",
    "state_groups = df.groupby('State') # run this in a cell\n",
    "\n",
    "# in another cell type your variable, add a . at the end, hit `tab`\n",
    "state_groups.\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "If you ran that code above you'd see you don't get the values you expected. That is because the column `'Value'` is not a numeric column.\n",
    "\n",
    "Run the following code to explore your __dataframe__ further:\n",
    "```python\n",
    "df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dtype` (or data type) of the `Value` column is _object_ which is a blanket term for \"anything non-numeric\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's clean up the __dataframe__ then! <br>\n",
    "[case study](02_case-study.ipynb)\n",
    "\n",
    "or go back to the\n",
    "[index](04_index.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
